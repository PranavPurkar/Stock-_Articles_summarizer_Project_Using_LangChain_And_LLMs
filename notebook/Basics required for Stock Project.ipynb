{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e07717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain-community in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (0.3.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.3.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.3.15)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.1.137)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b6ea4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'nvidia_text.txt'}, page_content='Nvidia Corporation[a][b] (/É›nËˆvÉªdiÉ™/, en-VID-ee-É™) is an American multinational corporation and technology company headquartered in Santa Clara, California, and incorporated in Delaware.[5] It is a software and fabless company which designs and supplies graphics processing units (GPUs), application programming interfaces (APIs) for data science and high-performance computing, as well as system on a chip units (SoCs) for the mobile computing and automotive market. Nvidia is also a dominant supplier of artificial intelligence (AI) hardware and software.[6][7][8]\\n\\nNvidia\\'s professional line of GPUs are used for edge-to-cloud computing and in supercomputers and workstations for applications in fields such as architecture, engineering and construction, media and entertainment, automotive, scientific research, and manufacturing design.[9] Its GeForce line of GPUs are aimed at the consumer market and are used in applications such as video editing, 3D rendering, and PC gaming. With a market share of 80.2% in the second quarter of 2023,[10] Nvidia leads the market for discrete desktop GPUs by a wide margin. The company expanded its presence in the gaming industry with the introduction of the Shield Portable (a handheld game console), Shield Tablet (a gaming tablet), and Shield TV (a digital media player), as well as its cloud gaming service GeForce Now.[11]\\n\\nIn addition to GPU design and outsourcing manufacturing, Nvidia provides the CUDA software platform and API that allows the creation of massively parallel programs which utilize GPUs.[12][13] They are deployed in supercomputing sites around the world.[14][15] In the late 2000s, Nvidia had moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets as well as vehicle navigation and entertainment systems.[16][17][18] Its competitors include AMD, Intel,[19] Qualcomm,[20] and AI accelerator companies such as Cerebras and Graphcore. It also makes AI-powered software for audio and video processing (e.g., Nvidia Maxine).[21]\\n\\nNvidia\\'s offer to acquire Arm from SoftBank in September 2020 failed to materialize following extended regulatory scrutiny, leading to the termination of the deal in February 2022 in what would have been the largest semiconductor acquisition.[22][23] In 2023, Nvidia became the seventh public U.S. company to be valued at over $1 trillion,[24] and the company\\'s valuation has skyrocketed since then as the company became a leader in data center chips with AI capabilities in the midst of the AI boom.[25][26] In June 2024, for one day, Nvidia overtook Microsoft as the world\\'s most valuable publicly traded company, with a market capitalization of over $3.3 trillion.[27]\\n\\nHistory\\nFounding\\n\\nThe Denny\\'s roadside diner in San Jose, California, c. 2023, where Nvidia\\'s three co-founders agreed to start the company in 1993\\n\\nAerial view of Endeavor, the first of the two new Nvidia headquarters buildings, in Santa Clara, California, in 2017. Apple Park is visible in the distance.\\n\\nEntrance of Endeavor headquarters building in 2018\\nNvidia was founded on April 5, 1993,[28][29][30] by Jensen Huang (CEO as of 2024), a Taiwanese-American electrical engineer who was previously the director of CoreWare at LSI Logic and a microprocessor designer at AMD; Chris Malachowsky, an engineer who worked at Sun Microsystems; and Curtis Priem, who was previously a senior staff engineer and graphics chip designer at IBM and Sun Microsystems.[31][32] The three men agreed to start the company in a meeting at a Denny\\'s roadside diner on Berryessa Road in East San Jose.[33][34]\\n\\nAt the time, Malachowsky and Priem were frustrated with Sun\\'s management and were looking to leave, but Huang was on \"firmer ground\",[35] in that he was already running his own division at LSI.[34] The three co-founders discussed a vision of the future which was so compelling that Huang decided to leave LSI[35] and become the chief executive officer of their new startup.[34]\\n\\nIn 1993, the three co-founders envisioned that the ideal trajectory for the forthcoming wave of computing would be in the realm of accelerated computing, specifically in graphics-based processing. This path was chosen due to its unique ability to tackle challenges that eluded general-purpose computing methods.[35] As Huang later explained: \"We also observed that video games were simultaneously one of the most computationally challenging problems and would have incredibly high sales volume. Those two conditions donâ€™t happen very often. Video games was our killer appâ€”a flywheel to reach large markets funding huge R&D to solve massive computational problems.\"[35] With $40,000 in the bank, the company was born.[35] The company subsequently received $20 million of venture capital funding from Sequoia Capital, Sutter Hill Ventures and others.[36]\\n\\nDuring the late 1990s, Nvidia was one of 70 startup companies chasing the idea that graphics acceleration for video games was the path to the future.[33] Only two survived: Nvidia and ATI Technologies, which merged into AMD.[33]\\n\\nNvidia initially had no name and the co-founders named all their files NV, as in \"next version\".[35] The need to incorporate the company prompted the co-founders to review all words with those two letters.[35] At one point, Malachowsky and Priem wanted to call the company NVision, but that name was already taken by a manufacturer of toilet paper.[34] Huang suggested the name Nvidia,[34] from \"invidia\", the Latin word for \"envy\".[35] The company\\'s original headquarters office was in Sunnyvale, California.[35]')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"nvidia_text.txt\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d3861f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.document_loaders.text.TextLoader"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8aebcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nvidia_text.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329a82b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2541"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(file_path=\"event_details.csv\",encoding=\"utf-8\")\n",
    "data = loader.load()\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee7d63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'event_details.csv', 'row': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4cde76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: unstructured in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (0.16.4)\n",
      "Collecting libmagic\n",
      "  Using cached libmagic-1.0-py3-none-any.whl\n",
      "Requirement already satisfied: python-magic in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (0.4.27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyhton-magic-bin (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for pyhton-magic-bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: unstructured in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (0.16.4)\n",
      "Requirement already satisfied: chardet in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (5.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (2.14.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (2024.10.22)\n",
      "Requirement already satisfied: langdetect in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (3.10.1)\n",
      "Requirement already satisfied: backoff in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (0.27.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (6.1.0)\n",
      "Requirement already satisfied: python-oxmsg in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (0.0.1)\n",
      "Requirement already satisfied: html5lib in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->unstructured) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from html5lib->unstructured) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Requirement already satisfied: olefile in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests->unstructured) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests->unstructured) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests->unstructured) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->unstructured) (0.4.6)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (43.0.3)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (0.2.0)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (0.27.2)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pydantic<2.10.0,>=2.9.2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (2.9.2)\n",
      "Requirement already satisfied: pypdf>=4.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (5.1.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (2.8.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#installing necessary libraries , libmagic is used for file type detection\n",
    "!pip3 install unstructured libmagic python-magic pyhton-magic-bin\n",
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9aad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0299886",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredURLLoader(urls= [\"https://www.techtarget.com/whatis/feature/Whats-going-on-with-Nvidia-stock-and-the-booming-AI-market\",\"https://economictimes.indiatimes.com/markets/stocks/earnings/samsung-posts-weaker-profit-recovery-as-it-misses-out-on-ai-boom/articleshow/114799168.cms?from=mdr\"])\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3fcda3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.techtarget.com/whatis/feature/Whats-going-on-with-Nvidia-stock-and-the-booming-AI-market'}, page_content=\"WhatIs\\n\\nBrowse Definitions :\\n\\nA\\n\\nB\\n\\nC\\n\\nD\\n\\nE\\n\\nF\\n\\nG\\n\\nH\\n\\nI\\n\\nJ\\n\\nK\\n\\nL\\n\\nM\\n\\nN\\n\\nO\\n\\nP\\n\\nQ\\n\\nR\\n\\nS\\n\\nT\\n\\nU\\n\\nV\\n\\nW\\n\\nX\\n\\nY\\n\\nZ\\n\\n#\\n\\nHome\\n\\nArtificial intelligence\\n\\nGetty Images/iStockphoto\\n\\nFeature\\n\\nWhat's going on with Nvidia stock and the booming AI market?\\n\\nNvidia's market value surpassed $3 trillion in 2024, fueled by the generative AI boom, rebounding tech sector and 154% stock growth in 2024. However, there are questions if AI will sustain the hype cycle.\\n\\nShare this item with your network:\\n\\nBy\\n\\nAmanda Hetler, Senior Editor\\n\\nPublished: 05 Nov 2024\\n\\nNvidia continues to grow as it surpassed the $3 trillion mark on June 18, 2024, before dipping just below $3 trillion at the end of August 2024. Nvidia is now the second largest-listed U.S. company based on market capitalization value behind Apple. In the middle of 2023, Nvidia passed the $1 trillion mark and was more valuable than Amazon and Alphabet, Google's parent company. In just nine months, the company's market worth increased from $1 trillion to $2 trillion in February 2024, and it took only three months to reach $3 trillion in June 2024.\\n\\nNvidia stock has seen its ups and downs. Even after reporting impressive growth numbers, Nvidia stock was down as much as 5% after its 2024 second-quarter earnings report. On Oct. 14, 2024, Nvidia stock closed at an all-time high of $138.07 due to the high demand for its GPUs needed for AI work. Nvidia’s newest chip, Blackwell, is so in demand that it is preordered and booked out for up to 12 months. Because of Nvidia’s steady growth, it will replace competitor Intel in the Dow Jones. S&P Global owns the Dow, and its committee selects stocks for the Dow based on how their industry is perceived to affect the U.S. economy.\\n\\nThe rise of Nvidia took some time. The tech sector faced a rough 2022. It started to rebound in 2023 despite tech layoffs. Generative AI is one of the main drivers of its rebound, and the stock market is showing signs of this rebound. The rise of generative AI led to a tech bull market, which is a time of expansion in the stock exchange.\\n\\nThe top-tier group of tech stocks is known as the Magnificent Seven and includes Alphabet, Amazon, Apple, Meta, Microsoft, Nvidia and Tesla. The Magnificent Seven company stocks were up an average of 111% in 2023, and Nvidia was up 239% in 2023.\\n\\nOn June 7, 2024, Nvidia stock completed a 10-for-1 stock split, which will make its stock price approximately $120 from $1,200. The shares will begin trading at the split-adjusted rates on June 10, 2024. Nvidia is splitting its stock to make it more accessible to employees and investors. This split will not change the overall value of the company. This means any stockholder who owned a share of stock before the split will receive nine more shares after the split. Over time, this lower stock price makes it easier for investors to access shares. This stock split helped Nvidia transition to the Dow Jones because the individual share price of the stock is a leading component of the Dow and not the company's total market value.\\n\\nEven with the daily ups and downs of the stock market, investors are taking notice of this growth and wondering how much the AI demand may drive the tech market in 2024.\\n\\nThe rise of Nvidia\\n\\nNvidia is one of the world's largest producers of GPUs. GPUs are computer chips or semiconductors that use math operations to produce visuals and images. The GPU manages and speeds up graphic workloads and displays visual content on a device such as PC or smartphone.\\n\\nEach of Nvidia's earnings reports exceeded expectations throughout 2023 as AI started gaining momentum and attention. Nvidia's advanced chips can use and process the massive amounts needed to train generative AI programs, such as ChatGPT and Gemini. Since Nvidia dominated this market before the increasing need for AI, Nvidia only grew larger as demand soared.\\n\\nNvidia reported revenue of $30 billion for its fiscal second quarter ending July 28, 2024. This revenue was up 15% from the previous quarter and up 152% from one year prior. The company earned record quarterly data center revenue of $26.3 billion, which increased 16% from the previous quarter and was up 154% from the previous year.\\n\\nTo put it in perspective, companies -- such as Apple and Microsoft -- are spending money on AI, and Nvidia is making money on AI as it produces the chips to run the technology.\\n\\nBusinesses need hardware to support high amounts of energy with the AI wave, but these advanced chips are also needed for the metaverse, gaming and spatial computing. Nvidia also makes chips for cars as technology continues to advance.\\n\\nThe core reasons for the rise of Nvidia stock\\n\\nThe rise of generative AI is one of the key factors of Nvidia's rise. However, there are some other reasons Nvidia stock increased significantly.\\n\\n1. The rise of supercomputers\\n\\nNvidia's chips power supercomputers because of the large amounts of data needed for this advanced technology. Supercomputing technology is used by companies, such as Meta for its AI Research SuperCluster computer, to train complex AI models. Tesla is also starting to build an AI-focused supercomputer for its vehicles.\\n\\n2. Demand for generative AI\\n\\nWith no signs of the demand for generative AI slowing down, Nvidia is poised to grow as each new system is adopted. The AI industry is expected to grow at a compound annual growth rate of 42% in the next 10 years, according to Bloomberg Intelligence. The demand for generative AI products could make the generative AI market worth $1.3 trillion by 2032.\\n\\nNvidia's A100 GPU chips are used in training the model for ChatGPT. Companies, such as OpenAI, that use large amounts of data for tasks such as training large language models evolve quickly and need more accelerated computing. The demand for GPUs will only grow as these systems train and evolve with more data.\\n\\n3. The changing world of the metaverse and XR\\n\\nNvidia has played a role in the metaverse and the virtual and augmented reality landscape with its Omniverse platform. Nvidia offers 3D modeling programs to help stream extended reality (XR) content efficiently. As the metaverse evolves, so does the demand for Nvidia chips to run the metaverse. Companies are turning to XR programs to create virtual environments for training.\\n\\nThe gaming world is also a large customer of Nvidia's graphics division. Video games need stronger cards to run the high-resolution images, especially as more games move to the cloud and away from the console. Nvidia's gaming GPUs, such as GeForce RTX 4070, help power video games at a higher resolution and quicker speed.\\n\\n4. Strategic placement\\n\\nNvidia is a large part of the cryptocurrency world. Miners use its cards for mining tokens, which requires a great deal of power. There was an unprecedented demand for Nvidia's cards as cryptocurrency took off.\\n\\nFuture of Nvidia\\n\\nWhile Nvidia's processors run most of the data centers that power generative AI, there are still some possible challenges ahead, including competition from tech giants to make their own AI chips, economic uncertainty and a rise in competition.\\n\\nThe generative AI market is expected to continue growing, but more rules and regulations are going to be introduced that could affect Nvidia's AI chips. The U.S. trade restrictions on China's advanced semiconductors are also impacting Nvidia's growth, as Chinese sales were a large part of Nvidia's data center sales.\\n\\nWith Nvidia's noticeable growth, other competitors are offering similar chips, such as AMD's Instinct MI200 family of GPU accelerators. Intel also launched a set of fifth-generation Intel Xeon processors for data centers. Companies may not want to rely on one company and will start exploring these other vendors, which could hurt Nvidia's growth.\\n\\nIt's hard to predict whether Nvidia will continue its growth at its current pace. Nvidia has built a solid foundation in the AI market, and if the generative AI market grows as predicted, its revenue will continue to grow. But it's unknown how much of the market Nvidia's competitors will take. Nvidia still has a strong share in the face of rising competition as it recently announced its H200 computing platform. Cloud providers such as Amazon, Google and Microsoft have developed AI processors, but they work with Nvidia chips.\\n\\nAnother obstacle facing Nvidia is the possibility of limiting sales of its advanced AI chips to some countries in the interest of national security.\\n\\nThe market is evolving quickly. Enterprises are looking to adopt generative AI, leading to the creation of new vendors to meet business needs. New areas such as security and compliance will also change the generative AI market in the business world.\\n\\nNvidia’s data center business is a leading driver of success and has a demand for AI infrastructure. Data center sales made up nearly 87% of Nvidia’s revenue. Other Big Tech companies -- such as Google, Microsoft and Meta -- continue to invest in AI and announced increased AI spending in their earnings reports. This shows that even if Nvidia’s stock doesn’t grow as rapidly as it once did, it doesn’t mean the company is performing poorly. It’s still growing and the need for its products remains strong.\\n\\nLearn more about other companies on the stock exchange innovating in AI.\\n\\nAmanda Hetler is senior editor and writer for WhatIs, where she writes technology explainer articles and works with freelancers.\\n\\nNext Steps\\n\\nInfrastructure for machine learning, AI requirements, examples\\n\\nHow to choose the best GPUs for AI projects\\n\\nRelated Resources\\n\\nAI in Human Resources: Revolutionizing Talent Acquisition and Management –Talk\\n\\nPersonalized BI Insights Using Artificial Intelligence –Talk\\n\\nBig Data Strategies for Cell Therapy Manufacturing –Talk\\n\\nHow AI Will Fully Automate the Lawn Mowing Business –Talk\\n\\nDig Deeper on Artificial intelligence\\n\\nIntel beats expectations, but AI chip Gaudi disappoints\\n\\nBy: Antone Gonsalves\\n\\nAI model training drives up demand for high-bandwidth memory\\n\\nBy: Adam Armstrong\\n\\nCribl positions for IPO with $319M in latest funding round\\n\\nBy: Eric Avidon\\n\\n10 top AI hardware and chip-making companies in 2024\\n\\nBy: Devin Partida\\n\\nSponsored News\\n\\nThe Future of Private AI: How Industries Take Advantage of AI –Equinix\\n\\nPrivate AI Demystified –Equinix\\n\\nFlexible IT: When Performance and Security Can’t Be Compromised –Dell Technologies\\n\\nRelated Content\\n\\nAMD, Intel and Nvidia's latest moves in the AI PC ... – Search Enterprise AI\\n\\nExplaining an AI bubble burst and what it could mean – WhatIs\\n\\n10 top AI hardware and chip-making companies in 2024 – Search Data Center\\n\\nSearch Networking\\n\\nWhat is a MAC address and how do I find it?\\n\\nA MAC address (media access control address) is a 12-digit hexadecimal number assigned to each device connected to the network.\\n\\nWhat is cloud networking?\\n\\nCloud networking is a type of IT infrastructure in which the cloud hosts some or all of an organization's networking resources.\\n\\nWhat is IPv6 (Internet Protocol version 6)?\\n\\nInternet Protocol version 6 (IPv6) is a set of specifications from the Internet Engineering Task Force (IETF) that is responsible...\\n\\nSecurity\\n\\nWhat is machine identity management?\\n\\nMachine identity management focuses on the machines connected to and accessing resources on a network.\\n\\nWhat is unified threat management (UTM)?\\n\\nUnified threat management (UTM) is an information security system that provides a single point of protection against cyberthreats...\\n\\nWhat is two-factor authentication (2FA)?\\n\\nTwo-factor authentication (2FA), sometimes referred to as two-step verification or dual-factor authentication, is a security ...\\n\\nSearch CIO\\n\\nWhat is a learning management system (LMS)?\\n\\nA learning management system (LMS) is a software application or web-based technology used to plan, implement and assess a ...\\n\\nWhat is a PMO (project management office)?\\n\\nA project management office (PMO) is a group, agency or department that defines and maintains the standards of project management...\\n\\nWhat are leadership skills?\\n\\nLeadership skills are the strengths and abilities individuals demonstrate that help to oversee processes, guide initiatives and ...\\n\\nSearch HRSoftware\\n\\nWhat is employee self-service (ESS)?\\n\\nEmployee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\n\\nWhat is DEI? Diversity, equity and inclusion explained\\n\\nDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\n\\nWhat is payroll software?\\n\\nPayroll software automates the process of paying salaried, hourly and contingent employees.\\n\\nSearch Customer Experience\\n\\nWhat is martech (marketing technology)?\\n\\nMartech (marketing technology) refers to the integration of software tools, platforms and applications designed to streamline and...\\n\\nWhat is customer insight (consumer insight)?\\n\\nCustomer insight, or consumer insight, is the understanding and interpretation of customer data, behaviors and feedback into ...\\n\\nWhat is market segmentation?\\n\\nMarket segmentation is a marketing strategy that uses well-defined criteria to divide a brand's total addressable market share ...\\n\\nBrowse by Topic\\n\\nBrowse Resources\\n\\nAbout Us\\n\\nMeet The Editors\\n\\nEditorial Ethics Policy\\n\\nContact Us\\n\\nAdvertisers\\n\\nBusiness Partners\\n\\nEvents\\n\\nMedia Kit\\n\\nCorporate Site\\n\\nReprints\\n\\nAll Rights Reserved, Copyright 1999 - 2024, TechTarget Privacy Policy Cookie Preferences Cookie Preferences Do Not Sell or Share My Personal Information\\n\\nClose\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45a83dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2231"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.[1] The largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering.[2] These models acquire predictive power regarding syntax, semantics, and ontologies[3] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.[4] The training compute of notable large models in FLOPs vs publication date over the period 2010-2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within leading companies (bottom right). The majority of these models are language models. The training compute of notable large AI models in FLOPs vs publication date over the period 2017-2024. The majority of large models are language models or multimodal models with language capacity.Before 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved then-SOTA (state of the art) perplexity.[5] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets , upon which they trained statistical language models.[7][8] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.[9]After neural networks became dominant in image processing around 2012,[10] they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before Transformers, it was done by seq2seq deep LSTM networks.\"\n",
    "len(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "718f0634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = text.split(\" \")\n",
    "len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "524c6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks =  []\n",
    "\n",
    "s =\"\"\n",
    "for it in word:\n",
    "    s += it + \" \" \n",
    "    if len(s)>200:\n",
    "        chunks.append(s)\n",
    "        s = \"\"\n",
    "chunks.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c790906c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical ',\n",
       " 'relationships from vast amounts of text during a self-supervised and semi-supervised training process.[1] The largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based ',\n",
       " 'architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering.[2] These models acquire predictive ',\n",
       " 'power regarding syntax, semantics, and ontologies[3] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.[4] The training compute ',\n",
       " 'of notable large models in FLOPs vs publication date over the period 2010-2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within ',\n",
       " 'leading companies (bottom right). The majority of these models are language models. The training compute of notable large AI models in FLOPs vs publication date over the period 2017-2024. The majority ',\n",
       " 'of large models are language models or multimodal models with language capacity.Before 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the ',\n",
       " 'IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved then-SOTA (state of the art) perplexity.[5] In the 2000s, as Internet ',\n",
       " 'use became prevalent, some researchers constructed Internet-scale language datasets , upon which they trained statistical language models.[7][8] In 2009, in most language processing tasks, statistical ',\n",
       " 'language models dominated over symbolic language models, as they can usefully ingest large datasets.[9]After neural networks became dominant in image processing around 2012,[10] they were applied to language ',\n",
       " 'modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before Transformers, it was done by seq2seq deep LSTM networks. ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38f7b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical \n",
      "215\n",
      "relationships from vast amounts of text during a self-supervised and semi-supervised training process.[1] The largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based \n",
      "208\n",
      "architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering.[2] These models acquire predictive \n",
      "206\n",
      "power regarding syntax, semantics, and ontologies[3] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.[4] The training compute \n",
      "204\n",
      "of notable large models in FLOPs vs publication date over the period 2010-2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within \n",
      "201\n",
      "leading companies (bottom right). The majority of these models are language models. The training compute of notable large AI models in FLOPs vs publication date over the period 2017-2024. The majority \n",
      "202\n",
      "of large models are language models or multimodal models with language capacity.Before 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the \n",
      "203\n",
      "IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved then-SOTA (state of the art) perplexity.[5] In the 2000s, as Internet \n",
      "201\n",
      "use became prevalent, some researchers constructed Internet-scale language datasets , upon which they trained statistical language models.[7][8] In 2009, in most language processing tasks, statistical \n",
      "208\n",
      "language models dominated over symbolic language models, as they can usefully ingest large datasets.[9]After neural networks became dominant in image processing around 2012,[10] they were applied to language \n",
      "173\n",
      "modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before Transformers, it was done by seq2seq deep LSTM networks. \n"
     ]
    }
   ],
   "source": [
    "for it in chunks:\n",
    "    print(len(it))\n",
    "    print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c7b6525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = \" \",\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba239870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning\n",
      "195\n",
      "statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.[1] The largest and most capable LLMs are artificial neural networks built with a\n",
      "190\n",
      "decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt\n",
      "197\n",
      "engineering.[2] These models acquire predictive power regarding syntax, semantics, and ontologies[3] inherent in human language corpora, but they also inherit inaccuracies and biases present in the\n",
      "195\n",
      "data on which they are trained.[4] The training compute of notable large models in FLOPs vs publication date over the period 2010-2024. For overall notable models (top left), frontier models (top\n",
      "198\n",
      "right), top language models (bottom left) and top models within leading companies (bottom right). The majority of these models are language models. The training compute of notable large AI models in\n",
      "196\n",
      "FLOPs vs publication date over the period 2017-2024. The majority of large models are language models or multimodal models with language capacity.Before 2017, there were a few language models that\n",
      "194\n",
      "were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words\n",
      "200\n",
      "achieved then-SOTA (state of the art) perplexity.[5] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets , upon which they trained statistical\n",
      "194\n",
      "language models.[7][8] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.[9]After neural\n",
      "200\n",
      "networks became dominant in image processing around 2012,[10] they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was\n",
      "63\n",
      "before Transformers, it was done by seq2seq deep LSTM networks.\n"
     ]
    }
   ],
   "source": [
    "for it in chunks:\n",
    "    print(len(it))\n",
    "    print(it)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0c80f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\",\"\\n\",\" \"],\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "chunksss = r_splitter.split_text(text)\n",
    "len(chunksss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874747a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1cba0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "195\n",
      "190\n",
      "197\n",
      "195\n",
      "198\n",
      "196\n",
      "194\n",
      "200\n",
      "194\n",
      "200\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "for it in chunks:\n",
    "    print(len(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "877acd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (3.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pranav\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b5395d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99ab85e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sample_text.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbce81c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meditation and yoga can improve mental health</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fruits, whole grains and vegetables helps control blood pressure</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These are the latest fashion trends for this week</td>\n",
       "      <td>Fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vibrant color jeans for male are becoming a trend</td>\n",
       "      <td>Fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The concert starts at 7 PM tonight</td>\n",
       "      <td>Event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Navaratri dandiya program at Expo center in Mumbai this october</td>\n",
       "      <td>Event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Exciting vacation destinations for your next trip</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maldives and Srilanka are gaining popularity in terms of low budget vacation places</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  text  \\\n",
       "0                                        Meditation and yoga can improve mental health   \n",
       "1                     Fruits, whole grains and vegetables helps control blood pressure   \n",
       "2                                    These are the latest fashion trends for this week   \n",
       "3                                    Vibrant color jeans for male are becoming a trend   \n",
       "4                                                   The concert starts at 7 PM tonight   \n",
       "5                      Navaratri dandiya program at Expo center in Mumbai this october   \n",
       "6                                    Exciting vacation destinations for your next trip   \n",
       "7  Maldives and Srilanka are gaining popularity in terms of low budget vacation places   \n",
       "\n",
       "  category  \n",
       "0   Health  \n",
       "1   Health  \n",
       "2  Fashion  \n",
       "3  Fashion  \n",
       "4    Event  \n",
       "5    Event  \n",
       "6   Travel  \n",
       "7   Travel  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52d2f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRANAV\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 768)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "vectors = encoder.encode(df.text)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf219dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = vectors.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2de51f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x000001CF9454E3D0> >"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a1ae850",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71a6e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"I want to enjoy my holidays\"\n",
    "vec = encoder.encode(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5f4b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "svec = np.array(vec).reshape(1,-1)\n",
    "svec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b7db104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances , I = index.search(svec,k=2)\n",
    "I.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "041bab96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Exciting vacation destinations for your next trip</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maldives and Srilanka are gaining popularity in terms of low budget vacation places</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  text  \\\n",
       "6                                    Exciting vacation destinations for your next trip   \n",
       "7  Maldives and Srilanka are gaining popularity in terms of low budget vacation places   \n",
       "\n",
       "  category  \n",
       "6   Travel  \n",
       "7   Travel  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59489c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
